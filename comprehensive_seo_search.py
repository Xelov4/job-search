#!/usr/bin/env python3
"""
Recherche SEO exhaustive avec toutes les variantes de mots-clÃ©s
Utilisant les opÃ©rateurs LinkedIn officiels
"""
import sys
sys.path.insert(0, 'src')

from linkedin_api import Linkedin
from dotenv import load_dotenv
import os
import json
import time
from datetime import datetime

load_dotenv('config/.env')

def comprehensive_seo_search():
    """Recherche exhaustive SEO avec variantes de mots-clÃ©s et opÃ©rateurs LinkedIn"""
    
    linkedin = Linkedin(
        os.getenv("LINKEDIN_EMAIL"), 
        os.getenv("LINKEDIN_PASSWORD"), 
        debug=False
    )
    
    print("ğŸ” RECHERCHE SEO EXHAUSTIVE - TOUTES LES VARIANTES")
    print("ğŸ“‹ BasÃ©e sur les opÃ©rateurs LinkedIn officiels")
    print("=" * 90)
    
    # DÃ©finir toutes les variantes SEO possibles
    seo_variations = {
        'exact_titles': [
            '"SEO Specialist"',
            '"SEO Manager"', 
            '"SEO Consultant"',
            '"Senior SEO Specialist"',
            '"SEO Expert"',
            '"Search Engine Optimization Specialist"',
            '"RÃ©fÃ©renceur SEO"',
            '"Chef de projet SEO"',
            '"SEO Content Manager"',
            '"Technical SEO Specialist"'
        ],
        'role_variants': [
            '("SEO Specialist" OR "SEO Manager" OR "SEO Consultant")',
            '("Senior SEO" OR "Lead SEO" OR "Head of SEO")',
            '("Search Engine Optimization" OR "rÃ©fÃ©renceur" OR "SEO")',
            '("Growth Manager" AND SEO)',
            '("Digital Marketing Manager" AND SEO)',
            '("Content Manager" AND SEO)',
            '("Marketing Manager" AND "search engine")'
        ],
        'comprehensive_search': [
            # Super recherche combinant tout
            '("SEO Specialist" OR "SEO Manager" OR "SEO Consultant" OR "rÃ©fÃ©renceur SEO" OR "Search Engine Optimization Specialist") NOT (stage OR junior OR alternance)',
            
            # Recherche par niveau d'expÃ©rience
            '(SEO OR rÃ©fÃ©renceur) AND (senior OR "chef de projet" OR manager OR lead OR head) NOT stage',
            
            # Recherche technique vs content
            '("Technical SEO" OR "SEO technique" OR "SEO Content" OR "Content SEO")',
            
            # Recherche par domaine
            'SEO AND (e-commerce OR ecommerce OR agency OR agence OR SaaS OR startup)',
            
            # Exclusion des faux positifs identifiÃ©s
            'SEO NOT (casino OR gaming OR "spontaneous application" OR "general manager" OR sales)'
        ],
        'growth_related': [
            # Variantes Growth qui peuvent inclure du SEO
            '"Growth Manager" AND (SEO OR "search engine" OR "organic traffic")',
            '"Performance Marketing" AND SEO',
            '"Digital Marketing" AND ("search engine optimization" OR SEO)',
            '"Acquisition Manager" AND (SEO OR "organic search")',
            '"Content Marketing" AND SEO'
        ]
    }
    
    all_results = {}
    total_jobs_found = 0
    
    # Tester chaque catÃ©gorie
    for category, searches in seo_variations.items():
        print(f"\nğŸ“‚ CATÃ‰GORIE : {category.upper().replace('_', ' ')}")
        print("-" * 70)
        
        category_results = {}
        
        for i, search_query in enumerate(searches, 1):
            print(f"\nğŸ” Test {i}/{len(searches)}: {search_query[:60]}{'...' if len(search_query) > 60 else ''}")
            
            try:
                # Effectuer la recherche
                jobs = linkedin.search_jobs(
                    keywords=search_query,
                    location="Paris, France",
                    limit=20
                )
                
                if jobs:
                    print(f"   âœ… {len(jobs)} emplois trouvÃ©s")
                    
                    # Analyse rapide de pertinence
                    relevant_count = 0
                    seo_in_title_count = 0
                    
                    for job in jobs[:10]:  # Analyser les 10 premiers
                        title = job.get('title', '').lower()
                        if any(term in title for term in ['seo', 'rÃ©fÃ©renceur', 'search engine', 'rÃ©fÃ©rencement']):
                            seo_in_title_count += 1
                        
                        # CritÃ¨res de pertinence plus larges
                        if any(term in title for term in ['seo', 'rÃ©fÃ©renceur', 'search engine', 'marketing', 'growth', 'content']):
                            relevant_count += 1
                    
                    print(f"   ğŸ“Š Top 10 - SEO dans titre: {seo_in_title_count}/10, Pertinents: {relevant_count}/10")
                    
                    # Afficher quelques titres intÃ©ressants
                    interesting_titles = []
                    for job in jobs[:5]:
                        title = job.get('title', '')
                        if any(term in title.lower() for term in ['seo', 'rÃ©fÃ©renceur', 'search engine']):
                            interesting_titles.append(title)
                    
                    if interesting_titles:
                        print(f"   ğŸ¯ Titres pertinents:")
                        for title in interesting_titles[:3]:
                            print(f"      â€¢ {title}")
                    
                    category_results[f"search_{i}"] = {
                        'query': search_query,
                        'count': len(jobs),
                        'seo_in_title_top10': seo_in_title_count,
                        'relevant_top10': relevant_count,
                        'jobs': jobs
                    }
                    
                    total_jobs_found += len(jobs)
                    
                else:
                    print(f"   âŒ Aucun rÃ©sultat")
                    category_results[f"search_{i}"] = {
                        'query': search_query,
                        'count': 0,
                        'jobs': []
                    }
                
                # Pause entre recherches
                time.sleep(2)
                
            except Exception as e:
                print(f"   âŒ Erreur: {e}")
                category_results[f"search_{i}"] = {
                    'query': search_query,
                    'error': str(e)
                }
        
        all_results[category] = category_results
    
    # Analyse comparative des catÃ©gories
    print(f"\n{'=' * 90}")
    print("ğŸ“Š RÃ‰SUMÃ‰ PAR CATÃ‰GORIE")
    print(f"{'=' * 90}")
    
    best_searches = []
    
    for category, results in all_results.items():
        valid_searches = [r for r in results.values() if 'count' in r and r['count'] > 0]
        
        if valid_searches:
            avg_results = sum(r['count'] for r in valid_searches) / len(valid_searches)
            avg_seo_in_title = sum(r.get('seo_in_title_top10', 0) for r in valid_searches) / len(valid_searches)
            avg_relevant = sum(r.get('relevant_top10', 0) for r in valid_searches) / len(valid_searches)
            
            print(f"\nğŸ“‚ {category.replace('_', ' ').upper()}:")
            print(f"   ğŸ“Š Recherches rÃ©ussies: {len(valid_searches)}")
            print(f"   ğŸ“ˆ Moyenne rÃ©sultats: {avg_results:.1f} emplois")
            print(f"   ğŸ¯ SEO dans titre (top 10): {avg_seo_in_title:.1f}/10")
            print(f"   âœ… Pertinence globale: {avg_relevant:.1f}/10")
            
            # Identifier les meilleures recherches de cette catÃ©gorie
            for search_key, search_data in results.items():
                if 'count' in search_data and search_data['count'] > 0:
                    score = (search_data.get('seo_in_title_top10', 0) * 2 + 
                            search_data.get('relevant_top10', 0)) / 3
                    
                    if score >= 3:  # Score significatif
                        best_searches.append({
                            'category': category,
                            'query': search_data['query'],
                            'score': score,
                            'count': search_data['count'],
                            'seo_in_title': search_data.get('seo_in_title_top10', 0)
                        })
    
    # Top des meilleures recherches
    print(f"\n{'=' * 90}")
    print("ğŸ† TOP DES MEILLEURES RECHERCHES")
    print(f"{'=' * 90}")
    
    best_searches.sort(key=lambda x: x['score'], reverse=True)
    
    for i, search in enumerate(best_searches[:5], 1):
        print(f"\n{i}. Score: {search['score']:.1f}/10")
        print(f"   ğŸ“‹ RequÃªte: {search['query']}")
        print(f"   ğŸ“Š RÃ©sultats: {search['count']} emplois")
        print(f"   ğŸ¯ SEO dans titre: {search['seo_in_title']}/10")
        print(f"   ğŸ“‚ CatÃ©gorie: {search['category']}")
    
    # Sauvegarder tous les rÃ©sultats
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"data/exports/comprehensive_seo_search_{timestamp}.json"
    
    # PrÃ©parer les donnÃ©es pour la sauvegarde
    save_data = {
        'search_timestamp': datetime.now().isoformat(),
        'total_searches': sum(len(cat_results) for cat_results in all_results.values()),
        'total_jobs_found': total_jobs_found,
        'categories': all_results,
        'best_searches': best_searches[:10],
        'search_parameters': {
            'location': "Paris, France",
            'limit_per_search': 20
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ RÃ©sultats complets sauvegardÃ©s: {filename}")
    print(f"ğŸ“Š Total: {sum(len(cat_results) for cat_results in all_results.values())} recherches")
    print(f"ğŸ“Š Emplois trouvÃ©s: {total_jobs_found}")
    
    # Recommandations finales
    print(f"\n{'=' * 90}")
    print("ğŸ’¡ RECOMMANDATIONS FINALES")
    print(f"{'=' * 90}")
    
    if best_searches:
        best_query = best_searches[0]['query']
        print(f"ğŸ† Meilleure requÃªte identifiÃ©e:")
        print(f"   {best_query}")
        print(f"   Score: {best_searches[0]['score']:.1f}/10")
        print(f"   SEO direct: {best_searches[0]['seo_in_title']}/10 dans le top 10")
        
        # Effectuer une extraction complÃ¨te avec la meilleure requÃªte
        print(f"\nğŸ”„ Lancement extraction complÃ¨te avec la meilleure requÃªte...")
        return run_full_extraction(best_query)
    else:
        print("âš ï¸ Aucune recherche n'a donnÃ© de rÃ©sultats suffisamment pertinents")
        return None

def extract_complete_job_info(job_details):
    """Extraction complÃ¨te des informations (version simplifiÃ©e)"""
    if not job_details or not isinstance(job_details, dict):
        return {}
    
    # Extraction des informations de base
    basic_info = {
        'title': job_details.get('title', 'N/A'),
        'job_id': job_details.get('jobPostingId', 'N/A'),
        'entity_urn': job_details.get('entityUrn', 'N/A'),
        'job_state': job_details.get('jobState', 'N/A')
    }
    
    # Extraction entreprise (simplifiÃ©e)
    company_details = job_details.get('companyDetails', {})
    company_info = {'name': 'N/A', 'logo_urls': []}
    
    if company_details:
        for key in company_details:
            if isinstance(company_details[key], dict):
                comp_result = company_details[key].get('companyResolutionResult', {})
                if isinstance(comp_result, dict) and 'name' in comp_result:
                    company_info['name'] = comp_result['name']
                    break
    
    # Description
    description = job_details.get('description', {})
    desc_text = ''
    if isinstance(description, dict):
        desc_text = description.get('text', '')
    elif isinstance(description, str):
        desc_text = description
    
    return {
        'basic_info': basic_info,
        'company': company_info,
        'description': {'text': desc_text},
        'workplace': {
            'formatted_location': job_details.get('formattedLocation', 'N/A'),
            'detailed_types': []
        }
    }

def run_full_extraction(best_query):
    """Effectue l'extraction complÃ¨te avec la meilleure requÃªte identifiÃ©e"""
    
    linkedin = Linkedin(
        os.getenv("LINKEDIN_EMAIL"), 
        os.getenv("LINKEDIN_PASSWORD"), 
        debug=False
    )
    
    print(f"\nğŸ¯ EXTRACTION COMPLÃˆTE AVEC LA MEILLEURE REQUÃŠTE")
    print(f"ğŸ“‹ RequÃªte: {best_query}")
    print("-" * 70)
    
    # Recherche avec la meilleure requÃªte
    search_results = linkedin.search_jobs(
        keywords=best_query,
        location="Paris, France",
        limit=20
    )
    
    if not search_results:
        print("âŒ Aucun rÃ©sultat pour l'extraction complÃ¨te")
        return None
    
    print(f"âœ… {len(search_results)} emplois trouvÃ©s pour extraction dÃ©taillÃ©e")
    
    all_jobs_complete = []
    
    # Extraction dÃ©taillÃ©e
    for i, job in enumerate(search_results, 1):
        print(f"\nğŸ“‹ EXTRACTION {i}/{len(search_results)}")
        
        basic_job_data = {
            'search_result_data': job,
            'detailed_data': None,
            'extraction_timestamp': datetime.now().isoformat(),
            'search_query_used': best_query
        }
        
        # RÃ©cupÃ©rer dÃ©tails
        if 'entityUrn' in job:
            job_id = job['entityUrn'].split(':')[-1]
            try:
                job_details = linkedin.get_job(job_id)
                if job_details:
                    complete_info = extract_complete_job_info(job_details)
                    basic_job_data['detailed_data'] = complete_info
                    
                    title = complete_info['basic_info']['title']
                    company = complete_info['company']['name']
                    print(f"   âœ… {title} - {company}")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"   âŒ Erreur: {e}")
                basic_job_data['error'] = str(e)
        
        all_jobs_complete.append(basic_job_data)
    
    # Sauvegarde extraction complÃ¨te
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"data/exports/best_seo_extraction_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_jobs_complete, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ Extraction complÃ¨te sauvegardÃ©e: {filename}")
    return filename

if __name__ == "__main__":
    result_file = comprehensive_seo_search()