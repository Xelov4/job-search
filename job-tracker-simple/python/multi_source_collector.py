"""
Multi-Source Job Collector
Collecte de jobs depuis multiple sources : LinkedIn Enhanced + Welcome to the Jungle
Int√©gration compl√®te avec le syst√®me Job Tracker existant
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
import logging
import os
import sys

# Imports du syst√®me existant
from job_data_types import JobOfferData
from linkedin_integration import LinkedInJobNormalizer
from wtj import scrape_wtj_fast
from supabase_client import SupabaseJobClient

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MultiSourceConfig:
    """Configuration pour collecte multi-sources"""
    keywords: str
    location: str = "Paris, √éle-de-France, France"
    
    # LinkedIn Enhanced
    enable_linkedin: bool = True
    linkedin_limit: int = 50
    
    # Welcome to the Jungle
    enable_wtj: bool = True
    wtj_max_pages: int = 3
    wtj_location: str = "√éle-de-France"
    
    # Autres sources futures
    enable_indeed: bool = False
    enable_glassdoor: bool = False
    
    # Param√®tres g√©n√©raux
    remove_duplicates: bool = True
    save_raw_results: bool = True
    auto_sync_supabase: bool = True


class MultiSourceJobCollector:
    """Collecteur unifi√© pour toutes les sources d'emploi"""
    
    def __init__(self, config: MultiSourceConfig):
        self.config = config
        self.collected_jobs: Dict[str, List[JobOfferData]] = {}
        self.all_jobs: List[JobOfferData] = []
        self.stats = {
            'total_collected': 0,
            'by_source': {},
            'duplicates_removed': 0,
            'sync_success': 0,
            'sync_errors': 0
        }
    
    async def collect_from_linkedin_enhanced(self) -> List[JobOfferData]:
        """Collecter jobs depuis LinkedIn Enhanced (workflow existant)"""
        if not self.config.enable_linkedin:
            logger.info("üì¥ LinkedIn d√©sactiv√© dans la config")
            return []
        
        logger.info(f"üîç Recherche LinkedIn Enhanced: '{self.config.keywords}'")
        
        try:
            # Chercher le dernier export LinkedIn Enhanced
            export_dir = "/root/Job/linkedin-mcp/data/exports"
            linkedin_files = []
            
            if os.path.exists(export_dir):
                for filename in os.listdir(export_dir):
                    if filename.startswith("enhanced_results_") and filename.endswith(".json"):
                        file_path = os.path.join(export_dir, filename)
                        linkedin_files.append((file_path, os.path.getmtime(file_path)))
            
            if not linkedin_files:
                logger.warning("‚ö†Ô∏è Aucun export LinkedIn Enhanced trouv√©")
                logger.info("üí° Lancez d'abord: python analyse_pertinence_complete_enhanced.py")
                return []
            
            # Prendre le fichier le plus r√©cent
            latest_file = max(linkedin_files, key=lambda x: x[1])[0]
            logger.info(f"üìÅ Utilisation export LinkedIn: {os.path.basename(latest_file)}")
            
            # Charger et normaliser les jobs LinkedIn
            with open(latest_file, 'r', encoding='utf-8') as f:
                linkedin_data = json.load(f)
            
            jobs = linkedin_data.get('jobs', [])
            normalized_jobs = []
            
            for job in jobs[:self.config.linkedin_limit]:
                normalized = LinkedInJobNormalizer.normalize_job(job)
                if normalized:
                    normalized_jobs.append(normalized)
            
            logger.info(f"‚úÖ LinkedIn Enhanced: {len(normalized_jobs)} jobs collect√©s")
            self.stats['by_source']['linkedin'] = len(normalized_jobs)
            return normalized_jobs
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte LinkedIn: {str(e)}")
            self.stats['by_source']['linkedin'] = 0
            return []
    
    async def collect_from_welcome_to_jungle(self) -> List[JobOfferData]:
        """Collecter jobs depuis Welcome to the Jungle"""
        if not self.config.enable_wtj:
            logger.info("üì¥ Welcome to the Jungle d√©sactiv√© dans la config")
            return []
        
        logger.info(f"üåü Recherche Welcome to the Jungle: '{self.config.keywords}'")
        
        try:
            jobs = await scrape_wtj_fast(
                keywords=self.config.keywords,
                location=self.config.wtj_location,
                max_pages=self.config.wtj_max_pages
            )
            
            logger.info(f"‚úÖ Welcome to the Jungle: {len(jobs)} jobs collect√©s")
            self.stats['by_source']['welcometothejungle'] = len(jobs)
            return jobs
            
        except Exception as e:
            logger.error(f"‚ùå Erreur collecte Welcome to the Jungle: {str(e)}")
            self.stats['by_source']['welcometothejungle'] = 0
            return []
    
    async def collect_from_indeed(self) -> List[JobOfferData]:
        """Collecter jobs depuis Indeed (√† impl√©menter)"""
        if not self.config.enable_indeed:
            return []
        
        logger.info("üöß Indeed - √Ä impl√©menter")
        # TODO: Impl√©menter Indeed scraper
        self.stats['by_source']['indeed'] = 0
        return []
    
    async def collect_from_glassdoor(self) -> List[JobOfferData]:
        """Collecter jobs depuis Glassdoor (√† impl√©menter)"""
        if not self.config.enable_glassdoor:
            return []
        
        logger.info("üöß Glassdoor - √Ä impl√©menter")
        # TODO: Impl√©menter Glassdoor scraper
        self.stats['by_source']['glassdoor'] = 0
        return []
    
    async def collect_from_all_sources(self) -> List[JobOfferData]:
        """Collecter jobs depuis toutes les sources activ√©es"""
        logger.info("üöÄ D√©but collecte multi-sources")
        logger.info(f"üéØ Recherche: '{self.config.keywords}' - {self.config.location}")
        
        # Collecte parall√®le de toutes les sources
        tasks = []
        
        if self.config.enable_linkedin:
            tasks.append(('linkedin', self.collect_from_linkedin_enhanced()))
        
        if self.config.enable_wtj:
            tasks.append(('welcometothejungle', self.collect_from_welcome_to_jungle()))
        
        if self.config.enable_indeed:
            tasks.append(('indeed', self.collect_from_indeed()))
        
        if self.config.enable_glassdoor:
            tasks.append(('glassdoor', self.collect_from_glassdoor()))
        
        # Ex√©cuter toutes les t√¢ches en parall√®le
        results = await asyncio.gather(*[task[1] for task in tasks], return_exceptions=True)
        
        # Traiter les r√©sultats
        all_jobs = []
        for i, (source_name, _) in enumerate(tasks):
            result = results[i]
            if isinstance(result, Exception):
                logger.error(f"‚ùå Erreur {source_name}: {str(result)}")
                self.collected_jobs[source_name] = []
            else:
                self.collected_jobs[source_name] = result
                all_jobs.extend(result)
                logger.info(f"‚úÖ {source_name}: {len(result)} jobs")
        
        # Supprimer les doublons si activ√©
        if self.config.remove_duplicates:
            all_jobs = self.remove_duplicates(all_jobs)
        
        self.all_jobs = all_jobs
        self.stats['total_collected'] = len(all_jobs)
        
        logger.info(f"üéØ Collecte termin√©e: {len(all_jobs)} jobs au total")
        return all_jobs
    
    def remove_duplicates(self, jobs: List[JobOfferData]) -> List[JobOfferData]:
        """Supprimer les doublons bas√©s sur titre + entreprise"""
        seen = set()
        unique_jobs = []
        duplicates_count = 0
        
        for job in jobs:
            # Cr√©er une cl√© unique bas√©e sur titre et entreprise (normalis√©s)
            title_clean = job.title.lower().strip()
            company_clean = job.company_name.lower().strip()
            unique_key = f"{title_clean}|{company_clean}"
            
            if unique_key not in seen:
                seen.add(unique_key)
                unique_jobs.append(job)
            else:
                duplicates_count += 1
        
        self.stats['duplicates_removed'] = duplicates_count
        logger.info(f"üßπ Doublons supprim√©s: {duplicates_count}")
        
        return unique_jobs
    
    async def sync_to_supabase(self) -> Dict[str, int]:
        """Synchroniser tous les jobs vers Supabase"""
        if not self.config.auto_sync_supabase:
            logger.info("üì¥ Sync Supabase d√©sactiv√© dans la config")
            return {'synced': 0, 'errors': 0}
        
        if not self.all_jobs:
            logger.warning("‚ö†Ô∏è Aucun job √† synchroniser")
            return {'synced': 0, 'errors': 0}
        
        logger.info(f"üóÑÔ∏è Synchronisation Supabase: {len(self.all_jobs)} jobs")
        
        try:
            supabase_client = SupabaseJobClient()
            sync_stats = supabase_client.save_jobs_batch(self.all_jobs)
            
            self.stats['sync_success'] = sync_stats.get('new', 0) + sync_stats.get('updated', 0)
            self.stats['sync_errors'] = sync_stats.get('errors', 0)
            
            logger.info(f"‚úÖ Sync termin√©e: {sync_stats}")
            return sync_stats
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sync Supabase: {str(e)}")
            self.stats['sync_errors'] = len(self.all_jobs)
            return {'synced': 0, 'errors': len(self.all_jobs)}
    
    def save_results(self, filename: Optional[str] = None) -> str:
        """Sauvegarder tous les r√©sultats en JSON"""
        if not self.config.save_raw_results:
            return ""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/root/Job/linkedin-mcp/data/exports/multi_source_collection_{timestamp}.json"
        
        # Pr√©parer les donn√©es d'export
        export_data = {
            'collection_metadata': {
                'timestamp': datetime.now().isoformat(),
                'config': asdict(self.config),
                'stats': self.stats
            },
            'jobs_by_source': {},
            'all_jobs_normalized': []
        }
        
        # Jobs par source (format brut)
        for source, jobs in self.collected_jobs.items():
            export_data['jobs_by_source'][source] = [asdict(job) for job in jobs]
        
        # Tous les jobs normalis√©s
        export_data['all_jobs_normalized'] = [asdict(job) for job in self.all_jobs]
        
        # Sauvegarder
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ R√©sultats sauvegard√©s: {filename}")
        return filename
    
    def print_summary(self):
        """Afficher un r√©sum√© de la collecte"""
        print("\n" + "="*60)
        print("üìä R√âSUM√â COLLECTE MULTI-SOURCES")
        print("="*60)
        print(f"üéØ Mots-cl√©s: {self.config.keywords}")
        print(f"üìç Localisation: {self.config.location}")
        print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nüìà STATISTIQUES GLOBALES:")
        print(f"  Total collect√©: {self.stats['total_collected']} jobs")
        print(f"  Doublons supprim√©s: {self.stats['duplicates_removed']}")
        print(f"  Sync Supabase: {self.stats['sync_success']} succ√®s, {self.stats['sync_errors']} erreurs")
        
        print(f"\nüóÇÔ∏è PAR SOURCE:")
        for source, count in self.stats['by_source'].items():
            status = "‚úÖ" if count > 0 else "‚ùå"
            print(f"  {status} {source.upper()}: {count} jobs")
        
        if self.all_jobs:
            print(f"\nüèÜ TOP 5 ENTREPRISES:")
            companies = {}
            for job in self.all_jobs:
                companies[job.company_name] = companies.get(job.company_name, 0) + 1
            
            for i, (company, count) in enumerate(sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]):
                print(f"  {i+1}. {company}: {count} jobs")
        
        print("\n" + "="*60)


# Fonctions principales pour utilisation
async def collect_jobs_multi_source(
    keywords: str,
    location: str = "Paris, √éle-de-France, France",
    enable_linkedin: bool = True,
    enable_wtj: bool = True,
    linkedin_limit: int = 50,
    wtj_max_pages: int = 3
) -> List[JobOfferData]:
    """
    Fonction principale pour collecte multi-sources
    """
    config = MultiSourceConfig(
        keywords=keywords,
        location=location,
        enable_linkedin=enable_linkedin,
        enable_wtj=enable_wtj,
        linkedin_limit=linkedin_limit,
        wtj_max_pages=wtj_max_pages,
        remove_duplicates=True,
        auto_sync_supabase=True,
        save_raw_results=True
    )
    
    collector = MultiSourceJobCollector(config)
    
    # Collecter de toutes les sources
    jobs = await collector.collect_from_all_sources()
    
    # Synchroniser vers Supabase
    await collector.sync_to_supabase()
    
    # Sauvegarder les r√©sultats
    collector.save_results()
    
    # Afficher le r√©sum√©
    collector.print_summary()
    
    return jobs


# Script principal
async def main():
    """Script principal de collecte multi-sources"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Collecte multi-sources de jobs")
    parser.add_argument("keywords", help="Mots-cl√©s de recherche")
    parser.add_argument("--location", default="Paris, √éle-de-France, France", help="Localisation")
    parser.add_argument("--no-linkedin", action="store_true", help="D√©sactiver LinkedIn")
    parser.add_argument("--no-wtj", action="store_true", help="D√©sactiver Welcome to the Jungle")
    parser.add_argument("--linkedin-limit", type=int, default=50, help="Limite jobs LinkedIn")
    parser.add_argument("--wtj-pages", type=int, default=3, help="Pages WTJ √† scraper")
    
    args = parser.parse_args()
    
    try:
        jobs = await collect_jobs_multi_source(
            keywords=args.keywords,
            location=args.location,
            enable_linkedin=not args.no_linkedin,
            enable_wtj=not args.no_wtj,
            linkedin_limit=args.linkedin_limit,
            wtj_max_pages=args.wtj_pages
        )
        
        print(f"\nüéâ Collecte termin√©e: {len(jobs)} jobs collect√©s et synchronis√©s!")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())