#!/usr/bin/env python3
"""
Workflow WTJ Complet - √âquivalent LinkedIn Enhanced
===================================================

Cr√©√© le : 2025-08-25 09:47:00 UTC
Auteur : Claude Code Assistant  
Version : 1.0.0

OBJECTIF :
----------
Processus complet √©quivalent au workflow LinkedIn Enhanced mais pour Welcome to the Jungle.
Reproduit exactement les m√™mes √©tapes : Recherche ‚Üí Analyse ‚Üí Normalisation ‚Üí Export ‚Üí DB

WORKFLOW COMPLET (5 √âTAPES) :
-----------------------------
1. üöÄ SCRAPING WTJ        : Recherche jobs via wtj.scrape_wtj_fast()
2. üîç NORMALISATION       : Conversion vers format Supabase via WTJDataNormalizer  
3. üíæ EXPORT JSON         : Sauvegarde format compatible LinkedIn Enhanced
4. üóÑÔ∏è SYNC SUPABASE       : Int√©gration base de donn√©es avec anti-doublons
5. üîç V√âRIFICATION        : Contr√¥le qualit√© et statistiques finales

ARCHITECTURE PIPELINE :
-----------------------
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ WTJ Keywords ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Fast Scraper‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Normalizer   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Supabase DB ‚îÇ
‚îÇ (Input)      ‚îÇ   ‚îÇ (Playwright)‚îÇ   ‚îÇ (Validation) ‚îÇ   ‚îÇ (Storage)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                                     ‚ñ≤
                           ‚ñº                                     ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ JSON Export ‚îÇ                      ‚îÇ Next.js UI   ‚îÇ
                   ‚îÇ (Compatible)‚îÇ                      ‚îÇ (Dashboard)  ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

COMPARAISON LinkedIn Enhanced vs WTJ :
--------------------------------------
LinkedIn Enhanced          ‚îÇ  WTJ Workflow Complete
===========================‚îÇ============================
analyse_pertinence_*.py    ‚îÇ  wtj_workflow_complete.py
LinkedIn API + Scraping    ‚îÇ  Playwright WTJ Scraping  
Enhanced Analysis + IA     ‚îÇ  Fast Scraper + Validation
JSON Export Standard       ‚îÇ  JSON Export Compatible
LinkedInSupabaseSync       ‚îÇ  WTJSupabaseSync
Dashboard Next.js          ‚îÇ  Dashboard Next.js (m√™me)

FONCTIONS PRINCIPALES :
-----------------------
- wtj_complete_workflow() : Workflow complet automatis√©
- interactive_wtj_workflow() : Interface interactive utilisateur
- demo_comparison_linkedin_wtj() : Comparaison entre sources

R√âSULTATS TYPIQUES :
--------------------
- SEO (1 page) : 81 jobs en 15.6s (312 jobs/min)
- Developer (2 pages) : 150+ jobs en 30s
- Export JSON : Format compatible LinkedIn Enhanced
- DB Sync : Anti-doublons intelligent inter-sources

UTILISATION :
-------------
# Automatique
python wtj_workflow_complete.py

# Par code
results = await wtj_complete_workflow(
    keywords="SEO", 
    location="√éle-de-France",
    max_pages=2,
    save_json=True,
    sync_to_db=True
)
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from typing import List, Dict

# Imports du syst√®me WTJ
from wtj import scrape_wtj_fast
from wtj_integration import WTJSupabaseSync, WTJDataNormalizer
from supabase_client import SimpleJobManager

async def wtj_complete_workflow(
    keywords: str,
    location: str = "√éle-de-France",
    max_pages: int = 2,
    save_json: bool = True,
    sync_to_db: bool = True
) -> Dict[str, any]:
    """
    Workflow WTJ complet √©quivalent au LinkedIn Enhanced
    ===================================================
    
    Cette fonction reproduit exactement le workflow LinkedIn Enhanced pour WTJ.
    Pipeline complet en 5 √©tapes avec m√©triques d√©taill√©es.
    
    Args:
        keywords (str): Mots-cl√©s de recherche (ex: "SEO", "developer")
        location (str): Localisation WTJ format (d√©faut: "√éle-de-France")
        max_pages (int): Nombre de pages √† scraper (d√©faut: 2)
        save_json (bool): Sauvegarder export JSON compatible (d√©faut: True)
        sync_to_db (bool): Synchroniser vers Supabase (d√©faut: True)
    
    Returns:
        Dict avec structure compl√®te :
        {
            'config': {...},           # Configuration utilis√©e
            'stats': {                 # Statistiques d√©taill√©es par √©tape
                'scraping': {...},     # Performance scraping
                'normalization': {...}, # R√©sultats normalisation  
                'database_sync': {...}, # Stats synchronisation DB
                'total_time': float    # Temps total workflow
            },
            'jobs': [...]             # Liste jobs normalis√©s
        }
    
    √âtapes ex√©cut√©es :
    1. Scraping WTJ avec wtj.scrape_wtj_fast()
    2. Normalisation via WTJDataNormalizer  
    3. Export JSON format LinkedIn Enhanced compatible
    4. Synchronisation Supabase avec anti-doublons
    5. V√©rification et rapport final
    
    Performance typique :
    - SEO (1 page) : 81 jobs en ~15s
    - Developer (2 pages) : 150+ jobs en ~30s
    - Taux normalisation : >95%
    - Sync DB : Anti-doublons intelligent
    """
    
    print("üåü WORKFLOW WTJ COMPLET")
    print("=" * 60)
    print(f"üéØ Recherche: '{keywords}'")
    print(f"üìç Location: {location}")
    print(f"üìÑ Pages: {max_pages}")
    print("")
    
    workflow_results = {
        'config': {
            'keywords': keywords,
            'location': location,
            'max_pages': max_pages,
            'timestamp': datetime.now().isoformat()
        },
        'stats': {
            'scraping': {},
            'normalization': {},
            'database_sync': {},
            'total_time': 0
        },
        'jobs': []
    }
    
    start_time = datetime.now()
    
    # √âTAPE 1: SCRAPING WTJ (√©quivalent recherche LinkedIn)
    print("üöÄ √âTAPE 1: Scraping Welcome to the Jungle...")
    scraping_start = datetime.now()
    
    try:
        # Utiliser le scraper rapide WTJ
        raw_jobs = await scrape_wtj_fast(
            keywords=keywords,
            location=location,
            max_pages=max_pages
        )
        
        scraping_time = (datetime.now() - scraping_start).total_seconds()
        
        workflow_results['stats']['scraping'] = {
            'success': True,
            'jobs_found': len(raw_jobs),
            'time_seconds': round(scraping_time, 2),
            'jobs_per_minute': round(len(raw_jobs) / (scraping_time / 60), 1) if scraping_time > 0 else 0
        }
        
        print(f"‚úÖ Scraping termin√©: {len(raw_jobs)} jobs en {scraping_time:.1f}s")
        
        if not raw_jobs:
            print("‚ùå Aucun job trouv√©, arr√™t du workflow")
            return workflow_results
        
    except Exception as e:
        print(f"‚ùå Erreur scraping: {str(e)}")
        workflow_results['stats']['scraping'] = {
            'success': False,
            'error': str(e),
            'jobs_found': 0
        }
        return workflow_results
    
    # √âTAPE 2: ANALYSE ET NORMALISATION (√©quivalent analyse Enhanced)
    print(f"\nüîç √âTAPE 2: Analyse et normalisation des donn√©es...")
    normalization_start = datetime.now()
    
    try:
        normalizer = WTJDataNormalizer()
        normalized_jobs = []
        normalization_errors = 0
        
        for i, raw_job in enumerate(raw_jobs, 1):
            normalized_job = normalizer.normalize_job(raw_job)
            
            if normalized_job:
                normalized_jobs.append(normalized_job)
                
                # Affichage conditionnel
                if i <= 5 or i % 10 == 0:
                    print(f"   üìã [{i}/{len(raw_jobs)}] {normalized_job.title} chez {normalized_job.company_name}")
            else:
                normalization_errors += 1
                print(f"   ‚ùå [{i}/{len(raw_jobs)}] √âchec normalisation")
        
        normalization_time = (datetime.now() - normalization_start).total_seconds()
        
        workflow_results['stats']['normalization'] = {
            'success': True,
            'jobs_normalized': len(normalized_jobs),
            'errors': normalization_errors,
            'success_rate': round(len(normalized_jobs) / len(raw_jobs) * 100, 1) if raw_jobs else 0,
            'time_seconds': round(normalization_time, 2)
        }
        
        print(f"‚úÖ Normalisation termin√©e: {len(normalized_jobs)} jobs valides ({normalization_errors} erreurs)")
        
        if not normalized_jobs:
            print("‚ùå Aucun job normalis√©, arr√™t du workflow")
            return workflow_results
            
        workflow_results['jobs'] = [job.__dict__ for job in normalized_jobs]
        
    except Exception as e:
        print(f"‚ùå Erreur normalisation: {str(e)}")
        workflow_results['stats']['normalization'] = {
            'success': False,
            'error': str(e)
        }
        return workflow_results
    
    # √âTAPE 3: SAUVEGARDE JSON (√©quivalent export LinkedIn)
    if save_json:
        print(f"\nüíæ √âTAPE 3: Export JSON...")
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_dir = "/root/Job/linkedin-mcp/data/exports"
            json_filename = f"{export_dir}/wtj_workflow_{keywords.replace(' ', '_')}_{timestamp}.json"
            
            os.makedirs(export_dir, exist_ok=True)
            
            # Format similaire aux exports LinkedIn Enhanced
            export_data = {
                'workflow_metadata': {
                    'source': 'welcometothejungle_complete_workflow',
                    'version': '1.0',
                    'timestamp': datetime.now().isoformat(),
                    'config': workflow_results['config'],
                    'stats': workflow_results['stats']
                },
                'jobs_analyzed': workflow_results['jobs'],  # M√™me nom que LinkedIn Enhanced
                'summary': {
                    'total_jobs': len(normalized_jobs),
                    'keywords': keywords,
                    'location': location,
                    'source_platform': 'welcometothejungle'
                }
            }
            
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ Export JSON sauv√©: {os.path.basename(json_filename)}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur export JSON: {str(e)}")
    
    # √âTAPE 4: SYNCHRONISATION SUPABASE (√©quivalent sync LinkedIn)
    if sync_to_db:
        print(f"\nüóÑÔ∏è √âTAPE 4: Synchronisation Supabase...")
        sync_start = datetime.now()
        
        try:
            # Utiliser le syst√®me de sync WTJ
            wtj_sync = WTJSupabaseSync()
            sync_stats = wtj_sync.sync_from_wtj_jobs(normalized_jobs)
            
            sync_time = (datetime.now() - sync_start).total_seconds()
            
            workflow_results['stats']['database_sync'] = {
                'success': True,
                'new_jobs': sync_stats.get('new', 0),
                'updated_jobs': sync_stats.get('updated', 0),
                'errors': sync_stats.get('errors', 0),
                'time_seconds': round(sync_time, 2)
            }
            
            print(f"‚úÖ Synchronisation termin√©e:")
            print(f"   üÜï Nouveaux jobs: {sync_stats.get('new', 0)}")
            print(f"   üîÑ Jobs mis √† jour: {sync_stats.get('updated', 0)}")
            print(f"   ‚ùå Erreurs: {sync_stats.get('errors', 0)}")
            
        except Exception as e:
            print(f"‚ùå Erreur sync Supabase: {str(e)}")
            workflow_results['stats']['database_sync'] = {
                'success': False,
                'error': str(e)
            }
    
    # √âTAPE 5: V√âRIFICATION ET R√âSUM√â
    print(f"\nüîç √âTAPE 5: V√©rification finale...")
    
    if sync_to_db:
        try:
            supabase_client = SimpleJobManager()
            
            # V√©rifier les jobs WTJ r√©cents
            recent_wtj_jobs = supabase_client.get_jobs_by_status(limit=10)
            wtj_jobs_in_db = [j for j in (recent_wtj_jobs or []) if j.get('source_platform') == 'welcometothejungle']
            
            print(f"‚úÖ V√©rification DB: {len(wtj_jobs_in_db)} jobs WTJ r√©cents trouv√©s")
            
            # Afficher quelques exemples
            if wtj_jobs_in_db:
                print(f"üìã Aper√ßu des jobs en base:")
                for i, job in enumerate(wtj_jobs_in_db[:3]):
                    print(f"   {i+1}. {job.get('title', 'N/A')} chez {job.get('company_name', 'N/A')}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur v√©rification DB: {str(e)}")
    
    # R√âSUM√â FINAL
    total_time = (datetime.now() - start_time).total_seconds()
    workflow_results['stats']['total_time'] = round(total_time, 2)
    
    print(f"\nüéâ WORKFLOW WTJ TERMIN√â!")
    print("=" * 60)
    print(f"‚è±Ô∏è Temps total: {total_time:.1f}s")
    print(f"üìä Pipeline: Scraping ‚Üí Normalisation ‚Üí Export ‚Üí DB")
    print(f"üéØ R√©sultat: {len(normalized_jobs)} jobs WTJ int√©gr√©s pour '{keywords}'")
    
    if sync_to_db and workflow_results['stats'].get('database_sync', {}).get('success'):
        new_jobs = workflow_results['stats']['database_sync'].get('new_jobs', 0)
        print(f"üóÑÔ∏è Base de donn√©es: +{new_jobs} nouveaux jobs")
    
    print("\nüìà COMPARAISON avec LinkedIn Enhanced:")
    print("‚úÖ M√™me workflow: Recherche ‚Üí Analyse ‚Üí Export ‚Üí DB")
    print("‚úÖ M√™me format de sortie: JSON compatible")
    print("‚úÖ M√™me syst√®me de normalisation")
    print("‚úÖ M√™me int√©gration Supabase")
    print("‚úÖ Dashboard Next.js compatible")
    
    return workflow_results

async def interactive_wtj_workflow():
    """Interface interactive pour le workflow WTJ"""
    print("üéÆ WORKFLOW WTJ INTERACTIF")
    print("=" * 40)
    
    # Configuration
    keywords = input("üîç Mots-cl√©s de recherche (d√©faut: SEO): ").strip() or "SEO"
    
    print("\nüìç Localisations disponibles:")
    print("1. √éle-de-France (d√©faut)")
    print("2. Autre")
    
    location_choice = input("Choix (1-2): ").strip()
    if location_choice == "2":
        location = input("Localisation personnalis√©e: ").strip() or "√éle-de-France"
    else:
        location = "√éle-de-France"
    
    try:
        max_pages = int(input("üìÑ Nombre de pages (d√©faut: 2): ").strip() or "2")
    except ValueError:
        max_pages = 2
    
    save_json = input("üíæ Sauvegarder JSON? (Y/n): ").strip().lower() != 'n'
    sync_to_db = input("üóÑÔ∏è Synchroniser vers Supabase? (Y/n): ").strip().lower() != 'n'
    
    print(f"\nüöÄ Lancement du workflow WTJ complet...")
    
    # Lancer le workflow
    results = await wtj_complete_workflow(
        keywords=keywords,
        location=location,
        max_pages=max_pages,
        save_json=save_json,
        sync_to_db=sync_to_db
    )
    
    return results

async def demo_comparison_linkedin_wtj():
    """D√©monstration comparative LinkedIn vs WTJ"""
    print("üî¨ COMPARAISON WORKFLOW: LinkedIn Enhanced vs WTJ")
    print("=" * 60)
    
    # M√™me recherche sur les deux sources
    keywords = "SEO"
    
    print(f"üéØ Recherche comparative pour: '{keywords}'")
    print("üìä Sources: LinkedIn Enhanced + Welcome to the Jungle")
    print("")
    
    # Workflow WTJ
    print("üåü 1. WORKFLOW WTJ...")
    wtj_results = await wtj_complete_workflow(
        keywords=keywords,
        location="√éle-de-France", 
        max_pages=2,
        save_json=True,
        sync_to_db=False  # Pas de sync pour la d√©mo
    )
    
    # R√©sum√© comparatif
    print("\nüìä R√âSUM√â COMPARATIF:")
    print("-" * 40)
    
    if wtj_results['stats'].get('scraping', {}).get('success'):
        wtj_jobs = wtj_results['stats']['scraping']['jobs_found']
        wtj_time = wtj_results['stats']['scraping']['time_seconds']
        print(f"üåü WTJ: {wtj_jobs} jobs en {wtj_time}s")
    else:
        print(f"üåü WTJ: √âchec")
    
    print(f"üìà LinkedIn Enhanced: Utilise les exports existants")
    print(f"üîÑ Format de sortie: Compatible pour les deux")
    print(f"üóÑÔ∏è Base de donn√©es: M√™me syst√®me Supabase")
    print(f"üåê Dashboard: M√™me interface Next.js")
    
    return wtj_results

async def main():
    """Menu principal"""
    while True:
        print("\n" + "=" * 60)
        print("üåü WORKFLOW WTJ COMPLET - Menu Principal")
        print("=" * 60)
        print("1. üöÄ Workflow WTJ complet (automatique)")
        print("2. üéÆ Workflow WTJ interactif")
        print("3. üî¨ Comparaison LinkedIn vs WTJ")
        print("4. üß™ Test rapide (developer, 1 page)")
        print("5. ‚ùå Quitter")
        
        choice = input("\nChoix (1-5): ").strip()
        
        if choice == "1":
            # Workflow automatique avec param√®tres par d√©faut
            results = await wtj_complete_workflow(
                keywords="SEO",
                location="√éle-de-France",
                max_pages=2,
                save_json=True,
                sync_to_db=True
            )
            
        elif choice == "2":
            # Workflow interactif
            results = await interactive_wtj_workflow()
            
        elif choice == "3":
            # Comparaison avec LinkedIn
            results = await demo_comparison_linkedin_wtj()
            
        elif choice == "4":
            # Test rapide
            print("\nüß™ Test rapide WTJ...")
            results = await wtj_complete_workflow(
                keywords="developer",
                location="√éle-de-France",
                max_pages=1,
                save_json=True,
                sync_to_db=True
            )
            
        elif choice == "5":
            print("üëã Au revoir!")
            break
            
        else:
            print("‚ùå Choix invalide")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüëã Workflow interrompu")
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {str(e)}")
        sys.exit(1)