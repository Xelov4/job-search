# üåü Welcome to the Jungle (WTJ) Scraper Module

Module Python int√©gr√© au **Job Tracker multi-sources** pour scraper et normaliser les offres d'emploi depuis [Welcome to the Jungle](https://www.welcometothejungle.com).

---

## üìã **Table des Mati√®res**

- [üöÄ Installation](#-installation)
- [‚ö° Usage Rapide](#-usage-rapide)
- [üèóÔ∏è Architecture](#%EF%B8%8F-architecture)
- [üìÇ Structure des Fichiers](#-structure-des-fichiers)
- [üîß API Reference](#-api-reference)
- [üß™ Tests et Validation](#-tests-et-validation)
- [üéØ Performances](#-performances)
- [üîç Troubleshooting](#-troubleshooting)

---

## üöÄ **Installation**

### **Pr√©requis**
```bash
# Installation des d√©pendances
cd /root/Job/linkedin-mcp
source venv/bin/activate
pip install playwright
playwright install chromium
```

### **D√©pendances**
- `playwright` : Web scraping headless
- `job_data_types.py` : Normalisation des donn√©es
- `supabase_client.py` : Int√©gration base de donn√©es

---

## ‚ö° **Usage Rapide**

### **1. Import Simple**
```python
from wtj import scrape_wtj_fast

# Scraping basique
jobs = await scrape_wtj_fast("SEO", "√éle-de-France", max_pages=2)
print(f"‚úÖ {len(jobs)} jobs trouv√©s")
```

### **2. Configuration Avanc√©e**
```python
from wtj.fast_scraper import FastWTJScraper, WTJFastConfig

config = WTJFastConfig(
    keywords="Data Analyst",
    location="√éle-de-France",
    max_pages=3,
    headless=True
)

async with FastWTJScraper(config) as scraper:
    jobs = await scraper.scrape_all_fast()
```

### **3. Workflow Complet avec DB**
```bash
cd /root/Job/linkedin-mcp/job-tracker-simple/python
python wtj/complete_test.py
```

---

## üèóÔ∏è **Architecture**

```mermaid
graph TB
    A[Welcome to the Jungle] --> B[FastWTJScraper]
    B --> C[Surface Scraping]
    C --> D[JobOfferData Normalization]
    D --> E[JSON Export]
    D --> F[Supabase Import]
    F --> G[Anti-Duplicate System]
    G --> H[Job Tracker Dashboard]
    
    B --> |Playwright| I[Headless Browser]
    C --> |CSS Selectors| J[Job Cards Extraction]
    D --> |Python Dataclass| K[Structured Data]
```

---

## üìÇ **Structure des Fichiers**

```
wtj/
‚îú‚îÄ‚îÄ __init__.py           # Module exports
‚îú‚îÄ‚îÄ README.md             # Cette documentation
‚îú‚îÄ‚îÄ fast_scraper.py       # üöÄ Scraper optimis√© (RECOMMAND√â)
‚îú‚îÄ‚îÄ scraper.py            # üìã Scraper complet avec deep analysis
‚îú‚îÄ‚îÄ complete_test.py      # üß™ Test workflow complet
‚îî‚îÄ‚îÄ db_test.py           # üîß Test import base de donn√©es
```

### **Fichiers Principaux**

| Fichier | Description | Usage |
|---------|-------------|--------|
| `fast_scraper.py` | **Scraper principal optimis√©** | Production, tests rapides |
| `scraper.py` | Scraper legacy avec deep scraping | D√©veloppement, analyse approfondie |
| `complete_test.py` | Test complet du workflow | Validation end-to-end |
| `db_test.py` | Test import simplifi√© | Debug connexion Supabase |

---

## üîß **API Reference**

### **FastWTJScraper**

#### **Configuration**
```python
@dataclass
class WTJFastConfig:
    keywords: str                    # Mots-cl√©s recherche
    location: str = "√éle-de-France" # Localisation
    max_pages: int = 2              # Pages √† scraper
    headless: bool = True           # Mode headless
```

#### **M√©thodes Principales**
```python
class FastWTJScraper:
    async def scrape_all_fast(self) -> List[JobOfferData]:
        """Scraping complet optimis√©"""
        
    async def scrape_page_fast(self, page: Page, page_num: int) -> List[Dict]:
        """Scraping d'une page sp√©cifique"""
        
    def build_search_url(self, page_num: int = 1) -> str:
        """Construction URL de recherche"""
```

### **Fonction Utilitaire**
```python
async def scrape_wtj_fast(
    keywords: str, 
    location: str = "√éle-de-France", 
    max_pages: int = 2
) -> List[JobOfferData]:
    """
    Fonction utilitaire pour scraping rapide
    
    Args:
        keywords: Mots-cl√©s de recherche
        location: Localisation (d√©faut: √éle-de-France)  
        max_pages: Nombre de pages √† scraper
        
    Returns:
        List[JobOfferData]: Jobs normalis√©s
    """
```

---

## üß™ **Tests et Validation**

### **1. Test Rapide**
```bash
# Test scraping seul
python wtj/fast_scraper.py
```

### **2. Test Base de Donn√©es**
```bash
# Test import Supabase
python wtj/db_test.py
```

### **3. Test Workflow Complet**
```bash
# Test end-to-end complet
python wtj/complete_test.py
```

### **4. Tests Int√©gr√©s**
```python
# Depuis le r√©pertoire parent
from wtj import scrape_wtj_fast

# Test basique
jobs = await scrape_wtj_fast("developer", max_pages=1)
assert len(jobs) > 0, "Aucun job trouv√©"

# Test avec Supabase
from supabase_client import SimpleJobManager, JobOfferData as SupabaseJobOfferData

client = SimpleJobManager()
success, message = client.save_job(supabase_job)
assert success, f"Import √©chou√©: {message}"
```

---

## üéØ **Performances**

### **Optimisations Appliqu√©es**

| Probl√®me | Solution | Am√©lioration |
|----------|----------|--------------|
| **Timeout (2+ minutes)** | Surface scraping au lieu de deep scraping | **~30 secondes** |
| **60+ requ√™tes individuelles** | Extraction directe depuis page de recherche | **2 requ√™tes seulement** |
| **Selectors fragiles** | S√©lecteurs multiples avec fallback | **95%+ de succ√®s** |

### **M√©triques Actuelles**
```bash
üìä PERFORMANCES WTJ FAST SCRAPER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üéØ R√©sultats typiques:
  ‚úÖ SEO (2 pages)      : 162 jobs en ~30s
  ‚úÖ Developer (1 page) : 81 jobs en ~15s  
  ‚úÖ Marketing (1 page) : 65 jobs en ~12s

‚ö° Performance:
  Temps moyen par page: 10-15 secondes
  Taux d'extraction: 95%+ 
  Jobs par minute: ~200
  Timeout: √âlimin√© ‚úÖ
```

---

## üîç **Troubleshooting**

### **Erreurs Communes**

#### **1. Playwright non install√©**
```bash
‚ùå ModuleNotFoundError: No module named 'playwright'
‚úÖ Solution:
   pip install playwright
   playwright install chromium
```

#### **2. Aucun job trouv√©**
```bash
‚ùå 0 jobs trouv√©s sur Welcome to the Jungle
‚úÖ Solutions:
   - Essayer des mots-cl√©s plus g√©n√©riques ("developer", "marketing")
   - V√©rifier la localisation (format: "√éle-de-France")
   - Tester avec headless=False pour voir la page
```

#### **3. Erreur Supabase**
```bash
‚ùå cannot import name 'SupabaseJobClient'
‚úÖ Solution:
   from supabase_client import SimpleJobManager
```

#### **4. Import Error**
```bash
‚ùå ImportError: cannot import name 'scrape_wtj_fast'
‚úÖ Solution:
   # Depuis le r√©pertoire job-tracker-simple/python
   from wtj import scrape_wtj_fast
```

### **Mode Debug**
```python
# Activer le mode non-headless pour debug
config = WTJFastConfig(
    keywords="SEO",
    location="√éle-de-France",
    headless=False  # Voir le navigateur
)
```

---

## üìä **Donn√©es Extraites**

### **Structure JobOfferData**
```python
JobOfferData(
    source_platform='welcometothejungle',
    source_id='wtj_12345',                    # ID unique g√©n√©r√©
    source_url='https://welcometothejungle.com/fr/jobs/...',
    title='SEO Specialist',                   # Titre extrait
    company_name='TechCorp',                  # Nom entreprise
    location='Paris, France',                 # Localisation normalis√©e
    description='Job description...',         # Description courte
    work_mode='remote',                       # remote/hybrid/on-site
    job_type='full-time',                     # Type de contrat
    application_url='https://...',            # URL candidature
    salary_info='',                           # Salaire (si disponible)
    posted_at=None,                           # Date publication
    discovered_at='2025-08-24T19:58:44'      # Date d√©couverte
)
```

---

## üîó **Int√©gration Syst√®me**

### **Multi-Source Collector**
```python
# Le module WTJ s'int√®gre dans le collecteur multi-sources
from wtj import scrape_wtj_fast
from multi_source_collector import collect_jobs_multi_source

# Usage int√©gr√©
jobs = await collect_jobs_multi_source(
    keywords="SEO",
    location="Paris, France",
    enable_linkedin=True,
    enable_wtj=True,        # ‚úÖ WTJ activ√©
    wtj_max_pages=2
)
```

### **Format Compatible LinkedIn Enhanced**
Le module WTJ produit exactement le m√™me format de donn√©es que LinkedIn Enhanced, permettant une int√©gration transparente dans le workflow existant.

---

## üéâ **Workflow Complet**

```mermaid
sequenceDiagram
    participant User
    participant WTJ as WTJ Scraper
    participant DB as Supabase DB
    participant UI as Next.js Dashboard

    User->>WTJ: scrape_wtj_fast("SEO")
    WTJ->>WTJ: Navigate to search page
    WTJ->>WTJ: Extract job cards (surface scraping)
    WTJ->>WTJ: Normalize to JobOfferData
    WTJ->>DB: Import jobs (anti-duplicate)
    DB->>UI: Display new jobs
    UI->>User: Job tracker updated ‚úÖ
```

---

## üìù **Changelog**

### **v1.0.0** (2025-08-24)
- ‚úÖ **Fast scraper** : √âlimination des timeouts
- ‚úÖ **Surface scraping** : Optimisation performance 
- ‚úÖ **Anti-duplicate** : Syst√®me de d√©tection doublons
- ‚úÖ **Supabase integration** : Import automatique
- ‚úÖ **Module organization** : Structure propre et r√©utilisable
- ‚úÖ **Test suite** : Tests complets et validation

---

## üéØ **Utilisation Recommand√©e**

```python
"""
EXEMPLE D'UTILISATION RECOMMAND√âE
================================
"""

import asyncio
from wtj import scrape_wtj_fast
from supabase_client import SimpleJobManager, JobOfferData as SupabaseJobOfferData

async def search_and_import_jobs():
    """Workflow recommand√© pour WTJ"""
    
    # 1. Scraping
    jobs = await scrape_wtj_fast("SEO", "√éle-de-France", max_pages=2)
    print(f"üéâ {len(jobs)} jobs trouv√©s")
    
    # 2. Import en base
    client = SimpleJobManager()
    imported = 0
    
    for job in jobs:
        supabase_job = SupabaseJobOfferData(
            source_platform=job.source_platform,
            source_id=job.source_id,
            source_url=job.source_url,
            title=job.title,
            company_name=job.company_name,
            company_url=None,
            location=job.location,
            description=job.description,
            work_mode=job.work_mode,
            job_type=job.job_type,
            application_url=job.application_url,
            salary_info=job.salary_info,
            posted_at=None,
            priority=0,
            notes=None
        )
        
        success, message = client.save_job(supabase_job)
        if success:
            imported += 1
    
    print(f"‚úÖ {imported} jobs import√©s en base")
    
    return jobs

# Usage
if __name__ == "__main__":
    asyncio.run(search_and_import_jobs())
```

---

*Module WTJ v1.0 - Int√©gr√© au Job Tracker Multi-Sources*  
*D√©velopp√© et test√© - Ao√ªt 2025* ‚úÖ